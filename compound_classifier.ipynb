{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\aditya joshi\\\\Desktop\\\\Machine Learning Project\\\\chemical classfier'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"C:\\Users\\aditya joshi\\Desktop\\Machine Learning Project\\chemical classfier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = pd.read_csv(\"musk_csv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>...</th>\n",
       "      <th>f158</th>\n",
       "      <th>f159</th>\n",
       "      <th>f160</th>\n",
       "      <th>f161</th>\n",
       "      <th>f162</th>\n",
       "      <th>f163</th>\n",
       "      <th>f164</th>\n",
       "      <th>f165</th>\n",
       "      <th>f166</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46</td>\n",
       "      <td>-108</td>\n",
       "      <td>-60</td>\n",
       "      <td>-69</td>\n",
       "      <td>-117</td>\n",
       "      <td>49</td>\n",
       "      <td>38</td>\n",
       "      <td>-161</td>\n",
       "      <td>-8</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>-308</td>\n",
       "      <td>52</td>\n",
       "      <td>-7</td>\n",
       "      <td>39</td>\n",
       "      <td>126</td>\n",
       "      <td>156</td>\n",
       "      <td>-50</td>\n",
       "      <td>-112</td>\n",
       "      <td>96</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41</td>\n",
       "      <td>-188</td>\n",
       "      <td>-145</td>\n",
       "      <td>22</td>\n",
       "      <td>-117</td>\n",
       "      <td>-6</td>\n",
       "      <td>57</td>\n",
       "      <td>-171</td>\n",
       "      <td>-39</td>\n",
       "      <td>-100</td>\n",
       "      <td>...</td>\n",
       "      <td>-59</td>\n",
       "      <td>-2</td>\n",
       "      <td>52</td>\n",
       "      <td>103</td>\n",
       "      <td>136</td>\n",
       "      <td>169</td>\n",
       "      <td>-61</td>\n",
       "      <td>-136</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46</td>\n",
       "      <td>-194</td>\n",
       "      <td>-145</td>\n",
       "      <td>28</td>\n",
       "      <td>-117</td>\n",
       "      <td>73</td>\n",
       "      <td>57</td>\n",
       "      <td>-168</td>\n",
       "      <td>-39</td>\n",
       "      <td>-22</td>\n",
       "      <td>...</td>\n",
       "      <td>-134</td>\n",
       "      <td>-154</td>\n",
       "      <td>57</td>\n",
       "      <td>143</td>\n",
       "      <td>142</td>\n",
       "      <td>165</td>\n",
       "      <td>-67</td>\n",
       "      <td>-145</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41</td>\n",
       "      <td>-188</td>\n",
       "      <td>-145</td>\n",
       "      <td>22</td>\n",
       "      <td>-117</td>\n",
       "      <td>-7</td>\n",
       "      <td>57</td>\n",
       "      <td>-170</td>\n",
       "      <td>-39</td>\n",
       "      <td>-99</td>\n",
       "      <td>...</td>\n",
       "      <td>-60</td>\n",
       "      <td>-4</td>\n",
       "      <td>52</td>\n",
       "      <td>104</td>\n",
       "      <td>136</td>\n",
       "      <td>168</td>\n",
       "      <td>-60</td>\n",
       "      <td>-135</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>-188</td>\n",
       "      <td>-145</td>\n",
       "      <td>22</td>\n",
       "      <td>-117</td>\n",
       "      <td>-7</td>\n",
       "      <td>57</td>\n",
       "      <td>-170</td>\n",
       "      <td>-39</td>\n",
       "      <td>-99</td>\n",
       "      <td>...</td>\n",
       "      <td>-60</td>\n",
       "      <td>-4</td>\n",
       "      <td>52</td>\n",
       "      <td>104</td>\n",
       "      <td>137</td>\n",
       "      <td>168</td>\n",
       "      <td>-60</td>\n",
       "      <td>-135</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 167 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   f1   f2   f3  f4   f5  f6  f7   f8  f9  f10  ...  f158  f159  f160  f161  \\\n",
       "0  46 -108  -60 -69 -117  49  38 -161  -8    5  ...  -308    52    -7    39   \n",
       "1  41 -188 -145  22 -117  -6  57 -171 -39 -100  ...   -59    -2    52   103   \n",
       "2  46 -194 -145  28 -117  73  57 -168 -39  -22  ...  -134  -154    57   143   \n",
       "3  41 -188 -145  22 -117  -7  57 -170 -39  -99  ...   -60    -4    52   104   \n",
       "4  41 -188 -145  22 -117  -7  57 -170 -39  -99  ...   -60    -4    52   104   \n",
       "\n",
       "   f162  f163  f164  f165  f166  class  \n",
       "0   126   156   -50  -112    96      1  \n",
       "1   136   169   -61  -136    79      1  \n",
       "2   142   165   -67  -145    39      1  \n",
       "3   136   168   -60  -135    80      1  \n",
       "4   137   168   -60  -135    80      1  \n",
       "\n",
       "[5 rows x 167 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = data_1.drop(\"ID\", axis=1)\n",
    "y= x.drop(\"molecule_name\", axis=1)\n",
    "z= y.drop(\"conformation_name\", axis=1)\n",
    "data = z\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f1       int64\n",
       "f2       int64\n",
       "f3       int64\n",
       "f4       int64\n",
       "f5       int64\n",
       "f6       int64\n",
       "f7       int64\n",
       "f8       int64\n",
       "f9       int64\n",
       "f10      int64\n",
       "f11      int64\n",
       "f12      int64\n",
       "f13      int64\n",
       "f14      int64\n",
       "f15      int64\n",
       "f16      int64\n",
       "f17      int64\n",
       "f18      int64\n",
       "f19      int64\n",
       "f20      int64\n",
       "f21      int64\n",
       "f22      int64\n",
       "f23      int64\n",
       "f24      int64\n",
       "f25      int64\n",
       "f26      int64\n",
       "f27      int64\n",
       "f28      int64\n",
       "f29      int64\n",
       "f30      int64\n",
       "         ...  \n",
       "f138     int64\n",
       "f139     int64\n",
       "f140     int64\n",
       "f141     int64\n",
       "f142     int64\n",
       "f143     int64\n",
       "f144     int64\n",
       "f145     int64\n",
       "f146     int64\n",
       "f147     int64\n",
       "f148     int64\n",
       "f149     int64\n",
       "f150     int64\n",
       "f151     int64\n",
       "f152     int64\n",
       "f153     int64\n",
       "f154     int64\n",
       "f155     int64\n",
       "f156     int64\n",
       "f157     int64\n",
       "f158     int64\n",
       "f159     int64\n",
       "f160     int64\n",
       "f161     int64\n",
       "f162     int64\n",
       "f163     int64\n",
       "f164     int64\n",
       "f165     int64\n",
       "f166     int64\n",
       "class    int64\n",
       "Length: 167, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f1       0\n",
       "f2       0\n",
       "f3       0\n",
       "f4       0\n",
       "f5       0\n",
       "f6       0\n",
       "f7       0\n",
       "f8       0\n",
       "f9       0\n",
       "f10      0\n",
       "f11      0\n",
       "f12      0\n",
       "f13      0\n",
       "f14      0\n",
       "f15      0\n",
       "f16      0\n",
       "f17      0\n",
       "f18      0\n",
       "f19      0\n",
       "f20      0\n",
       "f21      0\n",
       "f22      0\n",
       "f23      0\n",
       "f24      0\n",
       "f25      0\n",
       "f26      0\n",
       "f27      0\n",
       "f28      0\n",
       "f29      0\n",
       "f30      0\n",
       "        ..\n",
       "f138     0\n",
       "f139     0\n",
       "f140     0\n",
       "f141     0\n",
       "f142     0\n",
       "f143     0\n",
       "f144     0\n",
       "f145     0\n",
       "f146     0\n",
       "f147     0\n",
       "f148     0\n",
       "f149     0\n",
       "f150     0\n",
       "f151     0\n",
       "f152     0\n",
       "f153     0\n",
       "f154     0\n",
       "f155     0\n",
       "f156     0\n",
       "f157     0\n",
       "f158     0\n",
       "f159     0\n",
       "f160     0\n",
       "f161     0\n",
       "f162     0\n",
       "f163     0\n",
       "f164     0\n",
       "f165     0\n",
       "f166     0\n",
       "class    0\n",
       "Length: 167, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>...</th>\n",
       "      <th>f158</th>\n",
       "      <th>f159</th>\n",
       "      <th>f160</th>\n",
       "      <th>f161</th>\n",
       "      <th>f162</th>\n",
       "      <th>f163</th>\n",
       "      <th>f164</th>\n",
       "      <th>f165</th>\n",
       "      <th>f166</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>58.945135</td>\n",
       "      <td>-119.128524</td>\n",
       "      <td>-73.146560</td>\n",
       "      <td>-0.628372</td>\n",
       "      <td>-103.533495</td>\n",
       "      <td>18.359806</td>\n",
       "      <td>-14.108821</td>\n",
       "      <td>-1.858290</td>\n",
       "      <td>-86.003031</td>\n",
       "      <td>-44.495756</td>\n",
       "      <td>...</td>\n",
       "      <td>-184.798272</td>\n",
       "      <td>-75.795696</td>\n",
       "      <td>-26.073204</td>\n",
       "      <td>64.616702</td>\n",
       "      <td>112.037739</td>\n",
       "      <td>201.760230</td>\n",
       "      <td>-47.488330</td>\n",
       "      <td>-150.259927</td>\n",
       "      <td>41.770233</td>\n",
       "      <td>0.154138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>53.249007</td>\n",
       "      <td>90.813375</td>\n",
       "      <td>67.956235</td>\n",
       "      <td>80.444617</td>\n",
       "      <td>64.387559</td>\n",
       "      <td>80.593655</td>\n",
       "      <td>115.315673</td>\n",
       "      <td>90.372537</td>\n",
       "      <td>108.326676</td>\n",
       "      <td>72.088903</td>\n",
       "      <td>...</td>\n",
       "      <td>107.819514</td>\n",
       "      <td>127.861271</td>\n",
       "      <td>69.727964</td>\n",
       "      <td>100.861935</td>\n",
       "      <td>72.835040</td>\n",
       "      <td>59.526751</td>\n",
       "      <td>55.069365</td>\n",
       "      <td>76.019023</td>\n",
       "      <td>94.116085</td>\n",
       "      <td>0.361108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-31.000000</td>\n",
       "      <td>-199.000000</td>\n",
       "      <td>-167.000000</td>\n",
       "      <td>-114.000000</td>\n",
       "      <td>-118.000000</td>\n",
       "      <td>-183.000000</td>\n",
       "      <td>-171.000000</td>\n",
       "      <td>-225.000000</td>\n",
       "      <td>-245.000000</td>\n",
       "      <td>-286.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-328.000000</td>\n",
       "      <td>-219.000000</td>\n",
       "      <td>-136.000000</td>\n",
       "      <td>-120.000000</td>\n",
       "      <td>-69.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>-289.000000</td>\n",
       "      <td>-428.000000</td>\n",
       "      <td>-471.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>-193.000000</td>\n",
       "      <td>-137.000000</td>\n",
       "      <td>-70.000000</td>\n",
       "      <td>-117.000000</td>\n",
       "      <td>-28.000000</td>\n",
       "      <td>-159.000000</td>\n",
       "      <td>-85.000000</td>\n",
       "      <td>-217.000000</td>\n",
       "      <td>-96.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>-272.000000</td>\n",
       "      <td>-205.000000</td>\n",
       "      <td>-70.000000</td>\n",
       "      <td>-18.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>-68.000000</td>\n",
       "      <td>-179.000000</td>\n",
       "      <td>-9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>44.000000</td>\n",
       "      <td>-149.000000</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>-25.000000</td>\n",
       "      <td>-117.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>-40.000000</td>\n",
       "      <td>-29.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-234.000000</td>\n",
       "      <td>-131.000000</td>\n",
       "      <td>-21.000000</td>\n",
       "      <td>61.500000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>191.000000</td>\n",
       "      <td>-60.000000</td>\n",
       "      <td>-150.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>53.000000</td>\n",
       "      <td>-95.000000</td>\n",
       "      <td>-19.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>-116.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>-21.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-80.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>149.000000</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>215.000000</td>\n",
       "      <td>-45.000000</td>\n",
       "      <td>-120.000000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>292.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>325.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>220.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>231.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>179.000000</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>411.000000</td>\n",
       "      <td>355.000000</td>\n",
       "      <td>625.000000</td>\n",
       "      <td>295.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>367.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 167 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                f1           f2           f3           f4           f5  \\\n",
       "count  6598.000000  6598.000000  6598.000000  6598.000000  6598.000000   \n",
       "mean     58.945135  -119.128524   -73.146560    -0.628372  -103.533495   \n",
       "std      53.249007    90.813375    67.956235    80.444617    64.387559   \n",
       "min     -31.000000  -199.000000  -167.000000  -114.000000  -118.000000   \n",
       "25%      37.000000  -193.000000  -137.000000   -70.000000  -117.000000   \n",
       "50%      44.000000  -149.000000   -99.000000   -25.000000  -117.000000   \n",
       "75%      53.000000   -95.000000   -19.000000    42.000000  -116.000000   \n",
       "max     292.000000    95.000000    81.000000   161.000000   325.000000   \n",
       "\n",
       "                f6           f7           f8           f9          f10  ...  \\\n",
       "count  6598.000000  6598.000000  6598.000000  6598.000000  6598.000000  ...   \n",
       "mean     18.359806   -14.108821    -1.858290   -86.003031   -44.495756  ...   \n",
       "std      80.593655   115.315673    90.372537   108.326676    72.088903  ...   \n",
       "min    -183.000000  -171.000000  -225.000000  -245.000000  -286.000000  ...   \n",
       "25%     -28.000000  -159.000000   -85.000000  -217.000000   -96.750000  ...   \n",
       "50%      33.000000    27.000000    19.000000   -40.000000   -29.000000  ...   \n",
       "75%      74.000000    57.000000    61.000000   -21.000000     4.000000  ...   \n",
       "max     200.000000   220.000000   320.000000   147.000000   231.000000  ...   \n",
       "\n",
       "              f158         f159         f160         f161         f162  \\\n",
       "count  6598.000000  6598.000000  6598.000000  6598.000000  6598.000000   \n",
       "mean   -184.798272   -75.795696   -26.073204    64.616702   112.037739   \n",
       "std     107.819514   127.861271    69.727964   100.861935    72.835040   \n",
       "min    -328.000000  -219.000000  -136.000000  -120.000000   -69.000000   \n",
       "25%    -272.000000  -205.000000   -70.000000   -18.000000    71.000000   \n",
       "50%    -234.000000  -131.000000   -21.000000    61.500000   107.000000   \n",
       "75%     -80.000000    52.000000     9.000000   149.000000   129.000000   \n",
       "max      94.000000   179.000000   192.000000   411.000000   355.000000   \n",
       "\n",
       "              f163         f164         f165         f166        class  \n",
       "count  6598.000000  6598.000000  6598.000000  6598.000000  6598.000000  \n",
       "mean    201.760230   -47.488330  -150.259927    41.770233     0.154138  \n",
       "std      59.526751    55.069365    76.019023    94.116085     0.361108  \n",
       "min      73.000000  -289.000000  -428.000000  -471.000000     0.000000  \n",
       "25%     166.000000   -68.000000  -179.000000    -9.000000     0.000000  \n",
       "50%     191.000000   -60.000000  -150.000000    27.000000     0.000000  \n",
       "75%     215.000000   -45.000000  -120.000000   119.000000     0.000000  \n",
       "max     625.000000   295.000000   168.000000   367.000000     1.000000  \n",
       "\n",
       "[8 rows x 167 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='class', data=data)  # arguments are passed to np.histogram\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>...</th>\n",
       "      <th>f157</th>\n",
       "      <th>f158</th>\n",
       "      <th>f159</th>\n",
       "      <th>f160</th>\n",
       "      <th>f161</th>\n",
       "      <th>f162</th>\n",
       "      <th>f163</th>\n",
       "      <th>f164</th>\n",
       "      <th>f165</th>\n",
       "      <th>f166</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46</td>\n",
       "      <td>-108</td>\n",
       "      <td>-60</td>\n",
       "      <td>-69</td>\n",
       "      <td>-117</td>\n",
       "      <td>49</td>\n",
       "      <td>38</td>\n",
       "      <td>-161</td>\n",
       "      <td>-8</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>-244</td>\n",
       "      <td>-308</td>\n",
       "      <td>52</td>\n",
       "      <td>-7</td>\n",
       "      <td>39</td>\n",
       "      <td>126</td>\n",
       "      <td>156</td>\n",
       "      <td>-50</td>\n",
       "      <td>-112</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41</td>\n",
       "      <td>-188</td>\n",
       "      <td>-145</td>\n",
       "      <td>22</td>\n",
       "      <td>-117</td>\n",
       "      <td>-6</td>\n",
       "      <td>57</td>\n",
       "      <td>-171</td>\n",
       "      <td>-39</td>\n",
       "      <td>-100</td>\n",
       "      <td>...</td>\n",
       "      <td>-235</td>\n",
       "      <td>-59</td>\n",
       "      <td>-2</td>\n",
       "      <td>52</td>\n",
       "      <td>103</td>\n",
       "      <td>136</td>\n",
       "      <td>169</td>\n",
       "      <td>-61</td>\n",
       "      <td>-136</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46</td>\n",
       "      <td>-194</td>\n",
       "      <td>-145</td>\n",
       "      <td>28</td>\n",
       "      <td>-117</td>\n",
       "      <td>73</td>\n",
       "      <td>57</td>\n",
       "      <td>-168</td>\n",
       "      <td>-39</td>\n",
       "      <td>-22</td>\n",
       "      <td>...</td>\n",
       "      <td>-238</td>\n",
       "      <td>-134</td>\n",
       "      <td>-154</td>\n",
       "      <td>57</td>\n",
       "      <td>143</td>\n",
       "      <td>142</td>\n",
       "      <td>165</td>\n",
       "      <td>-67</td>\n",
       "      <td>-145</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41</td>\n",
       "      <td>-188</td>\n",
       "      <td>-145</td>\n",
       "      <td>22</td>\n",
       "      <td>-117</td>\n",
       "      <td>-7</td>\n",
       "      <td>57</td>\n",
       "      <td>-170</td>\n",
       "      <td>-39</td>\n",
       "      <td>-99</td>\n",
       "      <td>...</td>\n",
       "      <td>-236</td>\n",
       "      <td>-60</td>\n",
       "      <td>-4</td>\n",
       "      <td>52</td>\n",
       "      <td>104</td>\n",
       "      <td>136</td>\n",
       "      <td>168</td>\n",
       "      <td>-60</td>\n",
       "      <td>-135</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>-188</td>\n",
       "      <td>-145</td>\n",
       "      <td>22</td>\n",
       "      <td>-117</td>\n",
       "      <td>-7</td>\n",
       "      <td>57</td>\n",
       "      <td>-170</td>\n",
       "      <td>-39</td>\n",
       "      <td>-99</td>\n",
       "      <td>...</td>\n",
       "      <td>-236</td>\n",
       "      <td>-60</td>\n",
       "      <td>-4</td>\n",
       "      <td>52</td>\n",
       "      <td>104</td>\n",
       "      <td>137</td>\n",
       "      <td>168</td>\n",
       "      <td>-60</td>\n",
       "      <td>-135</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 166 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   f1   f2   f3  f4   f5  f6  f7   f8  f9  f10  ...  f157  f158  f159  f160  \\\n",
       "0  46 -108  -60 -69 -117  49  38 -161  -8    5  ...  -244  -308    52    -7   \n",
       "1  41 -188 -145  22 -117  -6  57 -171 -39 -100  ...  -235   -59    -2    52   \n",
       "2  46 -194 -145  28 -117  73  57 -168 -39  -22  ...  -238  -134  -154    57   \n",
       "3  41 -188 -145  22 -117  -7  57 -170 -39  -99  ...  -236   -60    -4    52   \n",
       "4  41 -188 -145  22 -117  -7  57 -170 -39  -99  ...  -236   -60    -4    52   \n",
       "\n",
       "   f161  f162  f163  f164  f165  f166  \n",
       "0    39   126   156   -50  -112    96  \n",
       "1   103   136   169   -61  -136    79  \n",
       "2   143   142   165   -67  -145    39  \n",
       "3   104   136   168   -60  -135    80  \n",
       "4   104   137   168   -60  -135    80  \n",
       "\n",
       "[5 rows x 166 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data.drop(\"class\", axis=1)\n",
    "Y = data['class']\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(166, 5278)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train.T \n",
    "X_test = X_test.T \n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1320)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y_train.shape\n",
    "Y_train = np.array(Y_train) \n",
    "Y_train=Y_train.reshape(-1,1)\n",
    "\n",
    "Y_train = Y_train.T \n",
    "# Y_train.shape\n",
    "Y_test = np.array(Y_test) \n",
    "Y_test=Y_test.reshape(-1,1)\n",
    "\n",
    "Y_test = Y_test.T \n",
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialize parameters / Define hyperparameters\n",
    "# 2. Loop for num_iterations:\n",
    "#     a. Forward propagation\n",
    "#     b. Compute cost function\n",
    "#     c. Backward propagation\n",
    "#     d. Update parameters (using parameters, and grads from backprop) \n",
    "# 4. Use trained parameters to predict labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/3985619/how-to-calculate-a-logistic-sigmoid-function-in-python\n",
    "#calculating logistic sigmoid function\n",
    "def sigmoid(z):\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intializing 2 layer neural network\n",
    "def initialize(n_x, n_h, n_y):\n",
    "\n",
    "    np.random.seed(2)\n",
    "    #W1 -- weight matrix of shape (n_h, n_x)\n",
    "    #b1 -- bias vector of shape (n_h, 1)\n",
    "    #W2 -- weight matrix of shape (n_y, n_h)\n",
    "    #b2 -- bias vector of shape (n_y, 1)\n",
    "    \n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.random.rand(n_h, 1)\n",
    "    W2 = np.random.rand(n_y, n_h) * 0.01\n",
    "    b2 = np.random.rand(n_y, 1)\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement the linear part of a layer's forward propagation\n",
    "def forward_prop(X, parameters):\n",
    "    #A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    #W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    #b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    #Returns:Z -- the input of the activation function, also called pre-activation parameter \n",
    "    #cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "\n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "\n",
    "    return A2, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement the cost function defined by equation\n",
    "def compute_cost(A2, Y, parameters):\n",
    "    \n",
    "    #Arguments:\n",
    "    #A2 -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    #Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "    \n",
    "    m = Y_train.shape[1]\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1 - A2), (1 - Y))\n",
    "    cost = - np.sum(logprobs) / m\n",
    "    cost = np.squeeze(cost)#To make sure your cost's shape is what we expect\n",
    "    #Returns: cost -- cross-entropy cost\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing backward propogation\n",
    "def back_prop(parameters, cache, X, Y):\n",
    "    #Arguments:\n",
    "    #parameters -- python dictionary containing our parameters \n",
    "    #cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    #X -- input data of shape (2, number of examples)\n",
    "    #Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    m = Y_train.shape[1]\n",
    "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "     # Retrieve also A1 and A2 from dictionary \"cache\".\n",
    "    A1 = cache['A1']\n",
    "    A2 = cache['A2']\n",
    "\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = (1 / m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.square(A1))\n",
    "    dW1 = (1 / m) * np.dot(dZ1, X.T)\n",
    "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    #Returns:grads -- python dictionary containing your gradients with respect to different parameters\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#updating parameters\n",
    "#updates parameters using the gradient descent update rule given below\n",
    "def update_params(parameters, grads, alpha):\n",
    "    #arguments:parameters -- python dictionary containing your parameters \n",
    "    #grads -- python dictionary containing your gradients\n",
    "    #Retrieve each parameter from the dictionary \"parameters\"\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    # Retrieve each gradient from the dictionary \"grads\"\n",
    "    dW1 = grads['dW1']\n",
    "    db1 = grads['db1']\n",
    "    dW2 = grads['dW2']\n",
    "    db2 = grads['db2']\n",
    "    # Update rule for each parameter\n",
    "    W1 = W1 - alpha * dW1\n",
    "    b1 = b1 - alpha * db1\n",
    "    W2 = W2 - alpha * dW2\n",
    "    b2 = b2 - alpha * db2\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X,Y):\n",
    "    A2, cache = forward_prop(X, parameters)\n",
    "    predictions = np.round(A2)\n",
    "    cost_test = compute_cost(A2, Y, parameters)\n",
    "    return predictions,cost_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, num_iters=2600, alpha=0.1, print_cost=True):\n",
    "    np.random.seed(3)\n",
    "    \n",
    "    parameters = initialize(n_x=X_train.shape[0],n_h= 120,n_y=Y_train.shape[0])\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "\n",
    "    costs_train = []\n",
    "    costs_test = []\n",
    "    accuracy_train = []\n",
    "    accuracy_test= []  \n",
    "    for i in range(0, num_iters):\n",
    "\n",
    "        A2, cache = forward_prop(X, parameters)\n",
    "\n",
    "        cost = compute_cost(A2, Y, parameters)\n",
    "#         print(cost.shape)\n",
    "        grads = back_prop(parameters, cache, X, Y)\n",
    "    \n",
    "        parameters = update_params(parameters, grads, alpha)\n",
    "        \n",
    "        if print_cost  and i % 100 == 0:\n",
    "            m=X.shape[1]\n",
    "            p=np.zeros((1,m))\n",
    "#             print (\"CostTrain after iteration %i: %f\" %(i, cost))\n",
    "            for x in range (0,A2.shape[1]):\n",
    "                if A2[0,x] >  0.5:\n",
    "                    p[0,x] = 1\n",
    "                else:\n",
    "                    p[0,x] = 0\n",
    "             \n",
    "            acc_train = np.sum((p==Y)/m)\n",
    "#             print (\"AccuracyTrain after iteration %i: %f\" %(i, acc_train ))\n",
    "            \n",
    "            \n",
    "            predictions,cost_test = predict(parameters, X_test,Y_test)\n",
    "#             print (\"CostTest after iteration %i: %f\" %(i, cost_test))\n",
    "            acc_test=float((np.dot(Y_test, predictions.T) + np.dot(1 - Y_test, 1 - predictions.T)) / float(Y_test.size))\n",
    "#             print (\"AccuracyTest after iteration %i: %f\" %(i, acc_test ))\n",
    "            \n",
    "            print('Iteration: %i Train Cost: %f || Test Cost: %f' %(i,cost,cost_test))\n",
    "            print('             Train Accuracy: %f || Test Accuracy: %f' %(cost,cost_test))\n",
    "            \n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs_train.append(cost)\n",
    "            costs_test.append(cost_test)\n",
    "            accuracy_train.append(acc_train)\n",
    "            accuracy_test.append(acc_test)\n",
    "            \n",
    "#     print(costs_train)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(np.squeeze(costs_train))\n",
    "    plt.plot(np.squeeze(costs_test))\n",
    "    plt.title('model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(accuracy_train)\n",
    "    plt.plot(accuracy_test)\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 Train Cost: 0.910155 || Test Cost: 0.113962\n",
      "             Train Accuracy: 0.910155 || Test Accuracy: 0.113962\n"
     ]
    }
   ],
   "source": [
    "parameters=model(X_train, Y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      1111\n",
      "           1       0.98      0.95      0.96       209\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      1320\n",
      "   macro avg       0.99      0.97      0.98      1320\n",
      "weighted avg       0.99      0.99      0.99      1320\n",
      "\n",
      "[[1107    4]\n",
      " [  11  198]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(Y_test.T,predictions.T))\n",
    "print(confusion_matrix(Y_test.T,predictions.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98%\n"
     ]
    }
   ],
   "source": [
    "predictions,cost_test = predict(parameters,X_test,Y_test)\n",
    "print ('Accuracy: %d' % float((np.dot(Y_test, predictions.T) + np.dot(1 - Y_test, 1 - predictions.T)) / float(Y_test.size) * 100) + '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
